{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BBC News RAG System\n",
        "\n",
        "This notebook implements a Retrieval-Augmented Generation (RAG) system for the BBC news dataset.\n",
        "\n",
        "## What is RAG?\n",
        "RAG combines:\n",
        "1. **Retrieval**: Finding relevant documents/articles from the dataset\n",
        "2. **Augmentation**: Adding retrieved context to the user's question\n",
        "3. **Generation**: Using an LLM to generate answers based on the retrieved context\n",
        "\n",
        "## System Components:\n",
        "- Document chunking and preprocessing\n",
        "- Vector embeddings using sentence-transformers\n",
        "- Vector store (FAISS or Chroma)\n",
        "- Retrieval system\n",
        "- Generation with OpenAI API\n",
        "- Question-answering pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Cyrus\\Documents\\Data Projects\\bbc nlp\\test_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ OpenAI API key loaded successfully\n",
            "Loaded 2225 articles from BBC dataset\n",
            "Categories: ['business' 'entertainment' 'politics' 'sport' 'tech']\n",
            "Loading sentence transformer model...\n",
            "✅ Sentence transformer model loaded\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from openai import OpenAI\n",
        "import json\n",
        "from typing import List, Dict, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up the API key\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    print(\"⚠️  WARNING: No OpenAI API key found!\")\n",
        "    print(\"Please set your API key: $env:OPENAI_API_KEY = 'your-api-key-here'\")\n",
        "    client = None\n",
        "else:\n",
        "    print(\"✅ OpenAI API key loaded successfully\")\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "# Load the full BBC dataset\n",
        "df = pd.read_csv(\"../data/raw_bbc.csv\")\n",
        "print(f\"Loaded {len(df)} articles from BBC dataset\")\n",
        "print(f\"Categories: {df['Category'].unique()}\")\n",
        "\n",
        "# Load the sentence transformer model\n",
        "print(\"Loading sentence transformer model...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast and efficient model\n",
        "print(\"✅ Sentence transformer model loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Document Preprocessing and Chunking\n",
        "\n",
        "We'll split long articles into smaller chunks to improve retrieval quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original article length: 2559 characters\n",
            "Number of chunks: 12\n",
            "First chunk: Ad sales boost Time Warner profit\n",
            "\n",
            "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\n",
            "\n",
            "The firm, which is now one o...\n",
            "Second chunk: sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit d...\n"
          ]
        }
      ],
      "source": [
        "# Function to split text into overlapping chunks\n",
        "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split text into overlapping chunks.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text to chunk\n",
        "        chunk_size: Maximum size of each chunk\n",
        "        overlap: Number of characters to overlap between chunks\n",
        "    \n",
        "    Returns:\n",
        "        List of text chunks\n",
        "    \"\"\"\n",
        "    if len(text) <= chunk_size:\n",
        "        return [text]\n",
        "    \n",
        "    chunks = []\n",
        "    start = 0\n",
        "    \n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        \n",
        "        # Try to break at sentence boundary\n",
        "        if end < len(text):\n",
        "            # Look for sentence endings within the last 100 characters\n",
        "            for i in range(min(100, chunk_size)):\n",
        "                if text[end - i] in '.!?':\n",
        "                    end = end - i + 1\n",
        "                    break\n",
        "        \n",
        "        chunk = text[start:end].strip()\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "        \n",
        "        start = end - overlap\n",
        "        \n",
        "        # Prevent infinite loop\n",
        "        if start >= len(text) - overlap:\n",
        "            break\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "\n",
        "def preprocess_articles(df: pd.DataFrame, chunk_size: int = 500) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Preprocess articles and create chunks for RAG.\n",
        "    \n",
        "    Returns:\n",
        "        List of dictionaries containing chunk information\n",
        "    \"\"\"\n",
        "    processed_chunks = []\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        text = row['Text']\n",
        "        category = row['Category']\n",
        "        filename = row.get('Filename', f'article_{idx}')\n",
        "        \n",
        "        # Create chunks\n",
        "        chunks = chunk_text(text, chunk_size=chunk_size)\n",
        "        \n",
        "        for chunk_idx, chunk in enumerate(chunks):\n",
        "            processed_chunks.append({\n",
        "                'article_id': idx,\n",
        "                'chunk_id': f\"{idx}_{chunk_idx}\",\n",
        "                'text': chunk,\n",
        "                'category': category,\n",
        "                'filename': filename,\n",
        "                'chunk_length': len(chunk)\n",
        "            })\n",
        "    \n",
        "    return processed_chunks\n",
        "\n",
        "# Test chunking on a sample article\n",
        "sample_text = df['Text'].iloc[0]\n",
        "print(f\"Original article length: {len(sample_text)} characters\")\n",
        "sample_chunks = chunk_text(sample_text, chunk_size=300)\n",
        "print(f\"Number of chunks: {len(sample_chunks)}\")\n",
        "print(f\"First chunk: {sample_chunks[0][:200]}...\")\n",
        "print(f\"Second chunk: {sample_chunks[1][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Vector Embeddings and Index\n",
        "\n",
        "We'll create embeddings for all text chunks and build a FAISS index for fast retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing sample of 50 articles for testing...\n",
            "Created 308 chunks from 50 articles\n",
            "Creating embeddings for 308 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 10/10 [00:02<00:00,  4.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created FAISS index with 308 vectors\n",
            "\n",
            "Chunk statistics:\n",
            "Average chunk length: 346.8 characters\n",
            "Min chunk length: 50 characters\n",
            "Max chunk length: 401 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def create_embeddings_and_index(chunks: List[Dict], model: SentenceTransformer) -> Tuple[np.ndarray, faiss.Index, List[Dict]]:\n",
        "    \"\"\"\n",
        "    Create embeddings for all chunks and build FAISS index.\n",
        "    \n",
        "    Returns:\n",
        "        embeddings: numpy array of embeddings\n",
        "        index: FAISS index for similarity search\n",
        "        chunk_metadata: metadata for each chunk\n",
        "    \"\"\"\n",
        "    print(f\"Creating embeddings for {len(chunks)} chunks...\")\n",
        "    \n",
        "    # Extract text for embedding\n",
        "    texts = [chunk['text'] for chunk in chunks]\n",
        "    \n",
        "    # Create embeddings\n",
        "    embeddings = model.encode(texts, show_progress_bar=True)\n",
        "    \n",
        "    # Create FAISS index\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dimension)  # Inner product (cosine similarity)\n",
        "    \n",
        "    # Normalize embeddings for cosine similarity\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    \n",
        "    # Add embeddings to index\n",
        "    index.add(embeddings.astype('float32'))\n",
        "    \n",
        "    print(f\"✅ Created FAISS index with {index.ntotal} vectors\")\n",
        "    \n",
        "    return embeddings, index, chunks\n",
        "\n",
        "# Process a sample of articles first (for testing)\n",
        "print(\"Processing sample of 50 articles for testing...\")\n",
        "sample_df = df.head(50)\n",
        "sample_chunks = preprocess_articles(sample_df, chunk_size=400)\n",
        "print(f\"Created {len(sample_chunks)} chunks from {len(sample_df)} articles\")\n",
        "\n",
        "# Create embeddings and index\n",
        "embeddings, index, chunk_metadata = create_embeddings_and_index(sample_chunks, model)\n",
        "\n",
        "print(f\"\\nChunk statistics:\")\n",
        "chunk_lengths = [chunk['chunk_length'] for chunk in chunk_metadata]\n",
        "print(f\"Average chunk length: {np.mean(chunk_lengths):.1f} characters\")\n",
        "print(f\"Min chunk length: {np.min(chunk_lengths)} characters\")\n",
        "print(f\"Max chunk length: {np.max(chunk_lengths)} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Retrieval System\n",
        "\n",
        "Implement the retrieval component that finds relevant chunks for a given question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing retrieval with question: 'What happened with Time Warner profits?'\n",
            "\n",
            "Retrieved 3 relevant chunks:\n",
            "\n",
            "Rank 1 (Score: 0.693):\n",
            "Category: business\n",
            "Text: Ad sales boost Time Warner profit\n",
            "\n",
            "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\n",
            "\n",
            "The firm, which is now one o...\n",
            "\n",
            "Rank 2 (Score: 0.678):\n",
            "Category: business\n",
            "Text: AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\n",
            "...\n",
            "\n",
            "Rank 3 (Score: 0.615):\n",
            "Category: business\n",
            "Text: rth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\n",
            "\n",
            "Time Warner said on Friday that it now ow...\n",
            "\n",
            "Formatted context:\n",
            "[Source 1] (Category: business, Score: 0.693)\n",
            "Ad sales boost Time Warner profit\n",
            "\n",
            "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\n",
            "\n",
            "The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn.\n",
            "\n",
            "\n",
            "[Source 2] (Category: business, Score: 0.678)\n",
            "AOL's existing custo...\n"
          ]
        }
      ],
      "source": [
        "def retrieve_relevant_chunks(question: str, index: faiss.Index, model: SentenceTransformer, \n",
        "                           chunk_metadata: List[Dict], k: int = 5) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Retrieve the most relevant chunks for a given question.\n",
        "    \n",
        "    Args:\n",
        "        question: User's question\n",
        "        index: FAISS index\n",
        "        model: Sentence transformer model\n",
        "        chunk_metadata: Metadata for each chunk\n",
        "        k: Number of chunks to retrieve\n",
        "    \n",
        "    Returns:\n",
        "        List of relevant chunks with metadata\n",
        "    \"\"\"\n",
        "    # Create embedding for the question\n",
        "    question_embedding = model.encode([question])\n",
        "    faiss.normalize_L2(question_embedding)\n",
        "    \n",
        "    # Search for similar chunks\n",
        "    scores, indices = index.search(question_embedding.astype('float32'), k)\n",
        "    \n",
        "    # Get relevant chunks\n",
        "    relevant_chunks = []\n",
        "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
        "        chunk_info = chunk_metadata[idx].copy()\n",
        "        chunk_info['similarity_score'] = float(score)\n",
        "        chunk_info['rank'] = i + 1\n",
        "        relevant_chunks.append(chunk_info)\n",
        "    \n",
        "    return relevant_chunks\n",
        "\n",
        "def format_context_for_llm(chunks: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Format retrieved chunks into context for the LLM.\n",
        "    \"\"\"\n",
        "    context_parts = []\n",
        "    \n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        context_parts.append(f\"[Source {i}] (Category: {chunk['category']}, Score: {chunk['similarity_score']:.3f})\")\n",
        "        context_parts.append(chunk['text'])\n",
        "        context_parts.append(\"\\n\")\n",
        "    \n",
        "    return \"\\n\".join(context_parts)\n",
        "\n",
        "# Test retrieval with a sample question\n",
        "test_question = \"What happened with Time Warner profits?\"\n",
        "print(f\"Testing retrieval with question: '{test_question}'\")\n",
        "\n",
        "relevant_chunks = retrieve_relevant_chunks(test_question, index, model, chunk_metadata, k=3)\n",
        "\n",
        "print(f\"\\nRetrieved {len(relevant_chunks)} relevant chunks:\")\n",
        "for chunk in relevant_chunks:\n",
        "    print(f\"\\nRank {chunk['rank']} (Score: {chunk['similarity_score']:.3f}):\")\n",
        "    print(f\"Category: {chunk['category']}\")\n",
        "    print(f\"Text: {chunk['text'][:200]}...\")\n",
        "\n",
        "print(f\"\\nFormatted context:\")\n",
        "print(format_context_for_llm(relevant_chunks)[:500] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Generation System\n",
        "\n",
        "Implement the generation component using OpenAI's API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What happened with Time Warner profits?\n",
            "\n",
            "Answer: Error generating answer: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-de9ab***********************5140. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
          ]
        }
      ],
      "source": [
        "def generate_answer(question: str, context: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
        "    \"\"\"\n",
        "    Generate an answer using OpenAI API with retrieved context.\n",
        "    \n",
        "    Args:\n",
        "        question: User's question\n",
        "        context: Retrieved context from documents\n",
        "        model: OpenAI model to use\n",
        "    \n",
        "    Returns:\n",
        "        Generated answer\n",
        "    \"\"\"\n",
        "    if not client:\n",
        "        return \"Error: OpenAI API client not available. Please set your API key.\"\n",
        "    \n",
        "    prompt = f\"\"\"You are a helpful assistant that answers questions based on the provided context from BBC news articles.\n",
        "\n",
        "Context from BBC News Articles:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Instructions:\n",
        "- Answer the question based ONLY on the information provided in the context above\n",
        "- If the context doesn't contain enough information to answer the question, say so\n",
        "- Be specific and cite relevant details from the context\n",
        "- If there are multiple sources, mention the different perspectives\n",
        "- Keep your answer concise but informative\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on BBC news articles.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=500,\n",
        "            temperature=0.1\n",
        "        )\n",
        "        \n",
        "        return response.choices[0].message.content.strip()\n",
        "    \n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer: {str(e)}\"\n",
        "\n",
        "# Test generation with our sample question\n",
        "if client:\n",
        "    context = format_context_for_llm(relevant_chunks)\n",
        "    answer = generate_answer(test_question, context)\n",
        "    \n",
        "    print(f\"Question: {test_question}\")\n",
        "    print(f\"\\nAnswer: {answer}\")\n",
        "else:\n",
        "    print(\"OpenAI client not available. Please set your API key to test generation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Complete RAG Pipeline\n",
        "\n",
        "Combine retrieval and generation into a complete RAG system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building RAG index...\n",
            "Created 308 chunks from 50 articles\n",
            "Creating embeddings for 308 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 10/10 [00:02<00:00,  4.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created FAISS index with 308 vectors\n",
            "✅ RAG index built successfully!\n",
            "\n",
            "RAG system ready! Index contains 308 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "class BBCRAGSystem:\n",
        "    \"\"\"\n",
        "    Complete RAG system for BBC news articles.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, df: pd.DataFrame, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        self.df = df\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.index = None\n",
        "        self.chunk_metadata = None\n",
        "        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\")) if os.getenv(\"OPENAI_API_KEY\") else None\n",
        "    \n",
        "    def build_index(self, chunk_size: int = 500, sample_size: int = None):\n",
        "        \"\"\"\n",
        "        Build the vector index from articles.\n",
        "        \"\"\"\n",
        "        print(f\"Building RAG index...\")\n",
        "        \n",
        "        # Use sample if specified\n",
        "        df_to_process = self.df.head(sample_size) if sample_size else self.df\n",
        "        \n",
        "        # Create chunks\n",
        "        chunks = preprocess_articles(df_to_process, chunk_size=chunk_size)\n",
        "        print(f\"Created {len(chunks)} chunks from {len(df_to_process)} articles\")\n",
        "        \n",
        "        # Create embeddings and index\n",
        "        embeddings, index, chunk_metadata = create_embeddings_and_index(chunks, self.model)\n",
        "        \n",
        "        self.index = index\n",
        "        self.chunk_metadata = chunk_metadata\n",
        "        \n",
        "        print(f\"✅ RAG index built successfully!\")\n",
        "        \n",
        "    def ask(self, question: str, k: int = 5, model: str = \"gpt-3.5-turbo\") -> Dict:\n",
        "        \"\"\"\n",
        "        Ask a question and get an answer using RAG.\n",
        "        \n",
        "        Returns:\n",
        "            Dictionary containing answer, sources, and metadata\n",
        "        \"\"\"\n",
        "        if self.index is None:\n",
        "            return {\"error\": \"Index not built. Please call build_index() first.\"}\n",
        "        \n",
        "        # Retrieve relevant chunks\n",
        "        relevant_chunks = retrieve_relevant_chunks(\n",
        "            question, self.index, self.model, self.chunk_metadata, k=k\n",
        "        )\n",
        "        \n",
        "        # Format context\n",
        "        context = format_context_for_llm(relevant_chunks)\n",
        "        \n",
        "        # Generate answer\n",
        "        if self.client:\n",
        "            answer = generate_answer(question, context, model)\n",
        "        else:\n",
        "            answer = \"OpenAI API not available. Please set your API key.\"\n",
        "        \n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"sources\": relevant_chunks,\n",
        "            \"context\": context,\n",
        "            \"num_sources\": len(relevant_chunks)\n",
        "        }\n",
        "    \n",
        "    def save_index(self, index_path: str, metadata_path: str):\n",
        "        \"\"\"\n",
        "        Save the FAISS index and metadata to disk.\n",
        "        \"\"\"\n",
        "        if self.index is None:\n",
        "            print(\"No index to save.\")\n",
        "            return\n",
        "        \n",
        "        faiss.write_index(self.index, index_path)\n",
        "        \n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(self.chunk_metadata, f, indent=2)\n",
        "        \n",
        "        print(f\"Index saved to {index_path}\")\n",
        "        print(f\"Metadata saved to {metadata_path}\")\n",
        "    \n",
        "    def load_index(self, index_path: str, metadata_path: str):\n",
        "        \"\"\"\n",
        "        Load the FAISS index and metadata from disk.\n",
        "        \"\"\"\n",
        "        self.index = faiss.read_index(index_path)\n",
        "        \n",
        "        with open(metadata_path, 'r') as f:\n",
        "            self.chunk_metadata = json.load(f)\n",
        "        \n",
        "        print(f\"Index loaded from {index_path}\")\n",
        "        print(f\"Metadata loaded from {metadata_path}\")\n",
        "\n",
        "# Initialize RAG system\n",
        "rag_system = BBCRAGSystem(df)\n",
        "\n",
        "# Build index with sample data (50 articles)\n",
        "rag_system.build_index(chunk_size=400, sample_size=50)\n",
        "\n",
        "print(f\"\\nRAG system ready! Index contains {rag_system.index.ntotal} chunks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test the RAG System\n",
        "\n",
        "Let's test our RAG system with various questions about the BBC news articles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Question 1: What happened with Time Warner profits?\n",
            "============================================================\n",
            "\n",
            "Answer:\n",
            "Error generating answer: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-de9ab***********************5140. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
            "\n",
            "Sources (3):\n",
            "\n",
            "1. business (Score: 0.693)\n",
            "   Ad sales boost Time Warner profit\n",
            "\n",
            "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from...\n",
            "\n",
            "2. business (Score: 0.678)\n",
            "   AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchang...\n",
            "\n",
            "3. business (Score: 0.615)\n",
            "   rth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users f...\n",
            "\n",
            "============================================================\n",
            "Question 2: What are the latest developments in technology?\n",
            "============================================================\n",
            "\n",
            "Answer:\n",
            "Error generating answer: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-de9ab***********************5140. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
            "\n",
            "Sources (3):\n",
            "\n",
            "1. business (Score: 0.405)\n",
            "   ust have all announced substantial investments in new printing plants....\n",
            "\n",
            "2. business (Score: 0.367)\n",
            "   likely to present advertising opportunities in the near future. The long-term outlook looks \"very favourable\" because of media and technology developm...\n",
            "\n",
            "3. business (Score: 0.317)\n",
            "   -boom generation in its retirement years than our economy has the capacity to deliver. If existing promises need to be changed, those changes should b...\n",
            "\n",
            "============================================================\n",
            "Question 3: What sports news is there?\n",
            "============================================================\n",
            "\n",
            "Answer:\n",
            "Error generating answer: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-de9ab***********************5140. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
            "\n",
            "Sources (3):\n",
            "\n",
            "1. business (Score: 0.354)\n",
            "   company now planned to observe the consultation procedures. The two Telegraph titles currently employ 521 journalists.\n",
            "\n",
            "Some broadsheet newspapers - e...\n",
            "\n",
            "2. business (Score: 0.331)\n",
            "   discuss how to react to the surprise announcement. The cuts come against a background of fierce competition for readers and sluggish advertising reven...\n",
            "\n",
            "3. business (Score: 0.325)\n",
            "   Telegraph newspapers axe 90 jobs\n",
            "\n",
            "The Daily and Sunday Telegraph newspapers are axing 90 journalist jobs - 17% of their editorial staff.\n",
            "\n",
            "The Telegrap...\n",
            "\n",
            "============================================================\n",
            "Question 4: What political events are happening?\n",
            "============================================================\n",
            "\n",
            "Answer:\n",
            "Error generating answer: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-de9ab***********************5140. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
            "\n",
            "Sources (3):\n",
            "\n",
            "1. business (Score: 0.350)\n",
            "   discuss how to react to the surprise announcement. The cuts come against a background of fierce competition for readers and sluggish advertising reven...\n",
            "\n",
            "2. business (Score: 0.341)\n",
            "   alue of the dollar and fears that Opec could rein in production to head off a seasonal drop in demand. Instability in Iraq and underlying fears about ...\n",
            "\n",
            "3. business (Score: 0.237)\n",
            "   trated the displeasure - as well as fears about further job losses as outsourcing takes hold. Among the new initiatives are the so-called \"one-euro jo...\n",
            "\n",
            "============================================================\n",
            "Question 5: What entertainment news is there?\n",
            "============================================================\n",
            "\n",
            "Answer:\n",
            "Error generating answer: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-de9ab***********************5140. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
            "\n",
            "Sources (3):\n",
            "\n",
            "1. business (Score: 0.405)\n",
            "   company now planned to observe the consultation procedures. The two Telegraph titles currently employ 521 journalists.\n",
            "\n",
            "Some broadsheet newspapers - e...\n",
            "\n",
            "2. business (Score: 0.395)\n",
            "   ages, or in some cases, have colour on every page. They are hoping that by boosting colour it will make their publications more attractive to advertis...\n",
            "\n",
            "3. business (Score: 0.390)\n",
            "   discuss how to react to the surprise announcement. The cuts come against a background of fierce competition for readers and sluggish advertising reven...\n"
          ]
        }
      ],
      "source": [
        "def test_rag_system(rag_system: BBCRAGSystem, questions: List[str]):\n",
        "    \"\"\"\n",
        "    Test the RAG system with a list of questions.\n",
        "    \"\"\"\n",
        "    for i, question in enumerate(questions, 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Question {i}: {question}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        result = rag_system.ask(question, k=3)\n",
        "        \n",
        "        if \"error\" in result:\n",
        "            print(f\"Error: {result['error']}\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\nAnswer:\")\n",
        "        print(result['answer'])\n",
        "        \n",
        "        print(f\"\\nSources ({result['num_sources']}):\")\n",
        "        for j, source in enumerate(result['sources'], 1):\n",
        "            print(f\"\\n{j}. {source['category']} (Score: {source['similarity_score']:.3f})\")\n",
        "            print(f\"   {source['text'][:150]}...\")\n",
        "\n",
        "# Test questions\n",
        "test_questions = [\n",
        "    \"What happened with Time Warner profits?\",\n",
        "    \"What are the latest developments in technology?\",\n",
        "    \"What sports news is there?\",\n",
        "    \"What political events are happening?\",\n",
        "    \"What entertainment news is there?\"\n",
        "]\n",
        "\n",
        "test_rag_system(rag_system, test_questions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Interactive Question-Answering\n",
        "\n",
        "Create an interactive interface for asking questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_qa(rag_system: BBCRAGSystem):\n",
        "    \"\"\"\n",
        "    Interactive question-answering interface.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BBC News RAG System - Interactive Mode\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Ask questions about BBC news articles!\")\n",
        "    print(\"Type 'quit' or 'exit' to stop.\")\n",
        "    print(\"Type 'help' for more options.\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    while True:\n",
        "        question = input(\"\\nYour question: \").strip()\n",
        "        \n",
        "        if question.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        \n",
        "        if question.lower() == 'help':\n",
        "            print(\"\\nAvailable commands:\")\n",
        "            print(\"- Ask any question about the news articles\")\n",
        "            print(\"- 'quit' or 'exit': Stop the program\")\n",
        "            print(\"- 'help': Show this help message\")\n",
        "            continue\n",
        "        \n",
        "        if not question:\n",
        "            continue\n",
        "        \n",
        "        print(\"\\nSearching for relevant information...\")\n",
        "        result = rag_system.ask(question, k=5)\n",
        "        \n",
        "        if \"error\" in result:\n",
        "            print(f\"Error: {result['error']}\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\nAnswer:\")\n",
        "        print(result['answer'])\n",
        "        \n",
        "        print(f\"\\nSources used ({result['num_sources']}):\")\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            print(f\"{i}. {source['category']} - Score: {source['similarity_score']:.3f}\")\n",
        "\n",
        "# Uncomment to start interactive mode\n",
        "# interactive_qa(rag_system)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Build Full Dataset Index\n",
        "\n",
        "Process the complete BBC dataset for production use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to build the full dataset index\n",
        "# print(\"Building full dataset index...\")\n",
        "# print(f\"This will process all {len(df)} articles\")\n",
        "# print(\"Estimated time: 10-20 minutes\")\n",
        "# \n",
        "# # Create new RAG system for full dataset\n",
        "# full_rag_system = BBCRAGSystem(df)\n",
        "# full_rag_system.build_index(chunk_size=500, sample_size=None)\n",
        "# \n",
        "# # Save the index\n",
        "# full_rag_system.save_index(\"../results/bbc_rag_index.faiss\", \"../results/bbc_rag_metadata.json\")\n",
        "# \n",
        "# print(\"\\n Full dataset index built and saved!\")\n",
        "# print(f\"Index contains {full_rag_system.index.ntotal} chunks from {len(df)} articles\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "test_env (3.11.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
